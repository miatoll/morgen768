<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenCV.js Object Recognition</title>
    <!-- Tailwind CSS for styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Google Fonts: Inter -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        /* Custom styles for the video and canvas */
        .video-container {
            position: relative;
            width: 100%;
            max-width: 720px;
            border-radius: 0.5rem;
            overflow: hidden;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }
        #video, #canvasOutput {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        #canvasOutput {
            z-index: 10;
        }
    </style>
</head>
<body class="bg-gray-900 text-white flex flex-col items-center justify-center min-h-screen p-4">

    <div class="w-full max-w-4xl mx-auto text-center">
        <h1 class="text-3xl md:text-4xl font-bold mb-2">Webcam Object Recognition</h1>
        <p class="text-gray-400 mb-6">Using OpenCV.js for real-time face detection.</p>

        <!-- Status display -->
        <div id="status" class="bg-gray-800 p-4 rounded-lg mb-6 text-left">
            <h2 class="font-semibold text-lg mb-2">Status</h2>
            <p id="status-text" class="text-gray-300">Initializing...</p>
        </div>

        <!-- Video and Canvas Container -->
        <div class="video-container mx-auto" style="padding-top: 56.25%;"> <!-- 16:9 Aspect Ratio -->
            <video id="video" playsinline autoplay muted></video>
            <canvas id="canvasOutput"></canvas>
        </div>

        <!-- Start Button -->
        <button id="startButton" class="mt-6 px-6 py-3 bg-blue-600 hover:bg-blue-700 text-white font-semibold rounded-lg shadow-md transition-transform transform hover:scale-105 disabled:opacity-50 disabled:cursor-not-allowed" disabled>
            Start Webcam
        </button>
    </div>

    <!-- OpenCV.js script -->
    <script async src="https://docs.opencv.org/4.5.4/opencv.js" onload="onOpenCvReady();"></script>

    <script type="text/javascript">
        // DOM Elements
        const video = document.getElementById('video');
        const canvasOutput = document.getElementById('canvasOutput');
        const canvasContext = canvasOutput.getContext('2d');
        const startButton = document.getElementById('startButton');
        const statusText = document.getElementById('status-text');

        let streaming = false;
        let cvReady = false;
        let faceCascade;
        let cap;
        let src;
        let gray;
        let faces;

        // --- 1. Initialization and Loading ---

        function updateStatus(text) {
            console.log(text);
            statusText.innerHTML = text;
        }

        // This function is called when the OpenCV.js script is loaded
        function onOpenCvReady() {
            updateStatus('OpenCV.js is ready. You can now start the webcam.');
            cvReady = true;
            startButton.disabled = false;
            loadHaarCascade();
        }

        // Load the pre-trained Haar Cascade model for face detection
        function loadHaarCascade() {
            updateStatus('Loading face detection model (haarcascade_frontalface_default.xml)...');
            const cascadeFile = 'haarcascade_frontalface_default.xml';
            const utils = new Utils('status-text');
            utils.createFileFromUrl(cascadeFile, `https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/${cascadeFile}`, () => {
                updateStatus('Face detection model loaded successfully.');
                faceCascade = new cv.CascadeClassifier();
                faceCascade.load(cascadeFile);
            });
        }

        // --- 2. Webcam and Video Handling ---

        startButton.addEventListener('click', () => {
            if (streaming) return;
            if (!cvReady) {
                alert("OpenCV is not ready yet. Please wait.");
                return;
            }
            if (!faceCascade) {
                alert("The face detection model is not loaded yet. Please wait.");
                return;
            }

            updateStatus('Requesting webcam access...');
            navigator.mediaDevices.getUserMedia({ video: true, audio: false })
                .then(function(stream) {
                    video.srcObject = stream;
                    video.play();
                })
                .catch(function(err) {
                    updateStatus(`Error accessing webcam: ${err}`);
                    console.error("An error occurred: " + err);
                });
        });

        // Listen for the 'canplay' event on the video to set up processing
        video.addEventListener('canplay', function() {
            if (!streaming) {
                // Set canvas and video dimensions
                const container = document.querySelector('.video-container');
                const w = container.clientWidth;
                const h = container.clientHeight;
                video.setAttribute('width', w);
                video.setAttribute('height', h);
                canvasOutput.setAttribute('width', w);
                canvasOutput.setAttribute('height', h);
                
                // Initialize OpenCV video capture
                cap = new cv.VideoCapture(video);
                src = new cv.Mat(h, w, cv.CV_8UC4);
                gray = new cv.Mat(h, w, cv.CV_8UC1);
                faces = new cv.RectVector();
                
                streaming = true;
                startButton.textContent = "Running...";
                startButton.disabled = true;
                updateStatus('Webcam started. Detecting faces...');
                
                // Start the processing loop
                requestAnimationFrame(processVideo);
            }
        }, false);

        // --- 3. Real-time Video Processing ---

        function processVideo() {
            if (!streaming) {
                // Clean up resources if streaming stops
                src.delete();
                gray.delete();
                faces.delete();
                return;
            }

            const startTime = performance.now();

            // Read a frame from the video
            cap.read(src);

            // Convert the frame to grayscale for the cascade classifier
            cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY, 0);

            // Detect faces
            try {
                faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0);
            } catch (err) {
                console.error("Error in detectMultiScale: ", err);
                updateStatus(`Detection error: ${err}`);
            }

            // Draw rectangles around detected faces
            canvasContext.clearRect(0, 0, canvasOutput.width, canvasOutput.height);
            for (let i = 0; i < faces.size(); ++i) {
                const rect = faces.get(i);
                const point1 = new cv.Point(rect.x, rect.y);
                const point2 = new cv.Point(rect.x + rect.width, rect.y + rect.height);
                // Draw a blue rectangle
                canvasContext.strokeStyle = 'rgba(0, 123, 255, 0.9)';
                canvasContext.lineWidth = 3;
                canvasContext.strokeRect(rect.x, rect.y, rect.width, rect.height);
            }
            
            const endTime = performance.now();
            const fps = 1000 / (endTime - startTime);
            updateStatus(`Detection running... Found ${faces.size()} faces. (FPS: ${fps.toFixed(1)})`);

            // Schedule the next frame processing
            requestAnimationFrame(processVideo);
        }

        // --- Helper class for loading files ---
        function Utils(statusId) {
            let self = this;
            this.statusElement = document.getElementById(statusId);

            this.createFileFromUrl = function(path, url, callback) {
                let request = new XMLHttpRequest();
                request.open('GET', url, true);
                request.responseType = 'arraybuffer';
                request.onload = function(ev) {
                    if (request.readyState === 4) {
                        if (request.status === 200) {
                            let data = new Uint8Array(request.response);
                            cv.FS_createDataFile('/', path, data, true, false, false);
                            callback();
                        } else {
                            self.printError('Failed to load ' + url + ' status: ' + request.status);
                        }
                    }
                };
                request.send();
            };

            this.printError = function(err) {
                if (typeof err === 'undefined') {
                    err = '';
                } else if (typeof err === 'number') {
                    if (!isNaN(err)) {
                        if (typeof cv !== 'undefined') {
                            err = 'Exception: ' + cv.exceptionFromPtr(err).msg;
                        }
                    }
                } else if (typeof err === 'string') {
                    // It's already a string
                } else if (err.toString) {
                    err = err.toString();
                }
                updateStatus(`ERROR: ${err}`);
            }
        }
    </script>
</body>
</html>
